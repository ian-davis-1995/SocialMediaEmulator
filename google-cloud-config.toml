[global]
# Project global settings related to every aspect of deploying gcp services.
region-name = "us-east1"
project-name = "afrl-il4-sbx-rhmindmodel-dk29"

[artifact-registry.docker]
# Settings related to building a docker image and uploading to a GCP artifact registry.
enabled = true
image-name = "social-media-emulator"
repository-name = "ians-docker-repository"
registry-region = "us-east1" # Optional, will use global if not specified

[cloud-run]
# Settings related to deploying a cloud run service pointed at a docker image in GCP artifact registry.
enabled = true
service-name = "social-media-emulator"
service-region = "us-east1" # Optional, will use global if not specified
require-authentication = false # Optional, will default to false it not specified. This setting configures whether the cloud run service will require IAM authentication to access.
ingress-control = "internal-and-cloud-load-balancing" # Optional, will default to "all" if not specified. See https://cloud.google.com/sdk/gcloud/reference/run/deploy#--ingress
cloud_run_args = [] # Optional list of args to pass to the cloud run deploy invocation. See https://cloud.google.com/sdk/gcloud/reference/run/deploy

[gcsfuse]
# Enabling this section will allow gcsfuse to network map a given GCS storage bucket into your docker container at a configured filesystem location.
# Note that cloud run services must be authenticated with a cloud storage bucket via the GCP Interface.
# The script cannot automatically link the two.
# You will need to deploy your service once, let it fail, then manually attach the cloud storage bucket to the service and redeploy.
# The configuration options below will be passed in to your docker image as build arguments.
# The argument names are as follows:
# ENABLE_GCSFUSE (value of 0/1 for false/true of enabled here).
# MNT_DIR (equivalent to mount-dir here).
# BUCKET (equivalent to storage-bucket here).
enabled = true
mount-dir = "/DigitalDeception/data/" # Specifies the location where the storage bucket will be mounted within the docker container.
storage-bucket = "digital-deception-data" # The name of the gcs storage bucket that will be mounted.

[big-query]
# Enabling this section will provide build args to your docker container that will facilitate usage of a bigquery dataset in your cloud run service
# The argument names in the docker container will be specified as a comment inline here.
enabled = false # Corresponds to argument ENABLE_BIGQUERY
# dataset = "my-dataset" # Specifies the name of the dataset to mount, corresponds to argument BIGQUERY_DATASET

[artifact-registry.python-package-index]
# Enabling this section will automatically provide an environment variable to your docker container
# of an authenticated url to allow pip installing packages from a given gcp python artifact registry.
# The docker build argument provided is ARTIFACT_URL. It should be specified within your docker container where needed by:
# ARG ARTIFACT_URL
# And when pip installing packages, add --extra-index-url ${ARTIFACT_URL}, like so:
# pip install -r requirements.txt --user --extra-index-url ${ARTIFACT_URL}
# For reference, the generated url will look like this:
# https://oauth2accesstoken:{access-token}@{index-region}-python.pkg.dev/{index-project}/{index-name}/simple/
# By default, it will use the command `gcloud auth print-access-token` to get a key that will authenticate with the index.
# If necessary, you can overwrite this behavior using the `access-token` field below.
enabled = true
index-name = "online-experiment-python-utilities"
index-region = "us-central1" # Optional, will use global if not specified
# index-project = "my-gcp-project" # Optional, will use global if not specified

[artifact-registry.python-package-deploy]
# Settings related to building a python package and publishing to an artifact registry.
enabled = false
# index-name = "online-experiment-python-utilities"
# index-region = "us-central1"
# index-project = "my-gcp-project" # Optional, will use global if not specified
